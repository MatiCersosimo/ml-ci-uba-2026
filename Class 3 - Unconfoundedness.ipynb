{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Class 3: Treatment Effects Estimation under Unconfoundedness\n",
        "\n",
        "- Unconfoundedness assumption.\n",
        "- Stratified estimator.\n",
        "- Propensity score.\n",
        "- IPW estimator.\n",
        "- Bootstrap.\n",
        "- Balance checks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74cda798",
      "metadata": {},
      "source": [
        "## Unconfoundedness Assumption"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23e74687",
      "metadata": {},
      "source": [
        "In our hypothetical scenario, we were assuming that we could run an RCT where we could randomly assign our variable of interest (late status of a delivery in our example). In the real world, however, this is rarely possible. Moreover, the independence assumption:\n",
        "\n",
        "$$\\{Y_i(0), Y_i(1)\\} \\perp\\!\\!\\!\\!\\perp W_i$$\n",
        "\n",
        "is usually hard to defend. Suppose we have customers who live in \"retirement\" areas, more isolated higher-income zones in contrast to  urban, busy zones. These customers tend to be higher income and place larger and heavier orders in the platform due to a lack of other alternatives, and are more profitable for us. However, these larger orders tend to take a longer time to fulfill and deliver, and so we would expect to see these customers receiving disproportionately more late deliveries. If we also consider the fact that there might be less shoppers willing to fulfill the order, the likelihood of lateness is even higher. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfbbd10f",
      "metadata": {},
      "source": [
        "We will, however, assume that there exists some set of *observable, pre-treatment covariates* $X_i$ such that, after conditioning on $X_i$, the treatment $W_i$ is as good as random. This weaker assumption is referred to as **Unconfoundedness**.\n",
        "\n",
        "$$\\{Y_i(0), Y_i(1)\\} \\perp\\!\\!\\!\\!\\perp W_i | X_i$$\n",
        "\n",
        "The covariates $X_i$ are typically referred to as **confounders**. Without accounting for them, our simple treatment effect estimators will be biased. Unconfoundedness is still an untestable assumption, but since it is much weaker than full independence, it is also more defensible."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36508c2e",
      "metadata": {},
      "source": [
        "## Stratified Estimator"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56668d34",
      "metadata": {},
      "source": [
        "How do we account or \"control\" for $X_i$ in our treatment effect estimation? In its purest sense, unconfoundedness means that we have separate RCTs for each different value of $X_i$. That is, within each value of $X_i$, the units in the control group are acceptable counterfactuals of the units in the treatment group. Intuitively, it seems that we can first estimate the average treatment effect for each different value of $X_i$, that is, the **conditional average treatment effect** and then aggregate them to estimate the average treatment effect. This is easiest to understand when $X_i$ is discrete, that is, $X_i \\in \\mathcal{X}$ with $|\\mathcal{X}| < \\infty$ and we can clearly define its segments or **strata** (in our example above, living in a retirement area or not). The **stratified estimator** is defined as:\n",
        "\n",
        "$$\\hat{\\tau}_{STRAT} = \\sum_{x \\in \\mathcal{X}} \\frac{n_x}{n} \\hat{\\tau}(x), \\qquad {\\hat{\\tau}}(x) = \\frac{1}{n_{x1}}\\sum_{X_i = x, W_i = 1} Y_i - \\frac{1}{n_{x0}}\\sum_{X_i=x, W_i = 0} Y_i$$\n",
        "\n",
        "This is essentially estimating, for each stratum $x$, the conditional average treatment effect using difference-in-means. Within each stratum, RCT assumptions hold, so difference-in-means is valid. Once we get valid estimates for each stratum, we aggregate them by weighting them according to the weight of the stratum in the sample, $\\frac{n_x}{n}$. It can be shown that, if $\\frac{n_x}{n}$ reasonably estimates the population weight, $\\hat{\\tau}_{STRAT}$ is unbiased for the average treatment effect $\\tau$.\n",
        "\n",
        "Remarkably, we are not restricted to use difference-in-means. $\\hat{\\tau}_{STRAT}$ remains a valid estimator for $\\tau$ as long as the method used to obtain the estimate for $\\tau(x)$ is valid. This includes using other covariates unrelated to the unconfoundedness for variance reduction in a linear regression setting!\n",
        "\n",
        "We can obtain an expression for the asymptotic variance of this estimator invoking the Central Limit Theorem:\n",
        "\n",
        "$$\\sqrt{n}(\\hat{\\tau}_{STRAT} - \\tau) \\implies \\mathcal{N}(0, \\mathbb{V}_{STRAT}), \\qquad \\mathbb{V}_{STRAT} = \\mathbb{V}[\\tau(X_i)] +\\mathbb{E}\\left[\\frac{\\sigma^2_1(X_i)}{e(X_i)} + \\frac{\\sigma^2_0(X_i)}{1-e(X_i)}\\right]$$\n",
        "\n",
        "where $\\sigma^2_1(X_i) = \\mathbb{V}[Y_i(1)|X_i]$, $\\sigma^2_0(X_i) = \\mathbb{V}[Y_i(0)|X_i]$ and $e(X_i) = \\mathbb{P}[W_i = 1|X_i]$. Note that the asymptotic variance does not depend on the number of strata!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f707683",
      "metadata": {},
      "source": [
        "## Propensity Score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96d29239",
      "metadata": {},
      "source": [
        "The above derivations work well when $X_i$ is discrete and the strata are well defined such as our example above. But in many cases, the confounders are continuous variables, and it will be impossible to estimate something exactly as $\\hat{\\tau}_{STRAT}$ defined above, as we will not have access to enough observations for each possible value of $X_i$, less so if $X_i$ is multi-dimensional (think about age, income, historical activity on the platform), \n",
        "\n",
        "Luckily for us, in such situations, it suffices to condition on the **propensity score**, $e(X_i)$. The propensity score is the probability of receiving treatment conditioning on $X_i$:\n",
        "\n",
        "$$e(x) = \\mathbb{P}[W_i = 1|X_i = x]$$\n",
        "\n",
        "It can be shown that:\n",
        "\n",
        "$$\\mathbb{P}[W_i = w|Y_i(w), e(X_i)] = \\mathbb{P}[W_i = w|X_i]$$\n",
        "\n",
        "The fact that it is enough to control for the propensity score makes obtaining a stratified estimator more appealing. We could partition our observations into strata delimited by the value of the propensity score and using our estimator $\\hat{\\tau}_{STRAT}$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1963f608",
      "metadata": {},
      "source": [
        "## Inverse-Propensity Weighting (IPW) Estimator"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d775e21",
      "metadata": {},
      "source": [
        "There is an alternative non-parametric estimator that uses propensity scores, simpler and less discretionary than the stratified estimator. This estimator is called **Inverse-Propensity Weighting (IPW)**, and we can think of it as a modification of the difference-in-means estimator where we weigh units differently based on the values of the propensity scores. The IPW estimator is defined as:\n",
        "\n",
        "$$\\hat{\\tau}^*_{IPW} = \\frac{1}{n}\\sum_{i} \\left( \\frac{W_i Y_i}{e(X_i)} - \\frac{(1-W_i)Y_i}{1-e(X_i)}\\right)$$\n",
        "\n",
        "We can also think of the IPW estimator as the average of \"scores\", $\\Gamma_{i, IPW^*}$ (this is useful for a number of reasons we will see later):\n",
        "\n",
        "$$\\hat{\\tau}^*_{IPW} = \\frac{1}{n}\\sum_{i} \\Gamma_{i, IPW^*}, \\qquad \\Gamma_{i, IPW^*} =\\left( \\frac{W_i Y_i}{e(X_i)} - \\frac{(1-W_i)Y_i}{1-e(X_i)}\\right)$$\n",
        "\n",
        "Let's see if this is a valid estimator for $\\tau$:\n",
        "\n",
        "\\begin{align*}\\\n",
        "\\mathbb{E}[\\hat{\\tau}^*_{IPW}] &= \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i} \\Gamma_{i, IPW^*}\\right]\\\\\n",
        "\\mathbb{E}[\\hat{\\tau}^*_{IPW}] &= \\mathbb{E}\\left[\\frac{W_i Y_i}{e(X_i)} - \\frac{(1-W_i)Y_i}{1-e(X_i)}\\right]\\\\\n",
        "\\mathbb{E}[\\hat{\\tau}^*_{IPW}] &= \\mathbb{E}\\left[\\frac{W_i Y_i(1)}{e(X_i)} - \\frac{(1-W_i)Y_i(0)}{1-e(X_i)}\\right]\\\\\n",
        "\\end{align*}\n",
        "\n",
        "Now let's use the unconfoundedness assumption:\n",
        "\n",
        "\\begin{align*}\\\n",
        "\\mathbb{E}[\\hat{\\tau}^*_{IPW}] &= \\mathbb{E}\\left[\\frac{W_i Y_i(1)}{e(X_i)} - \\frac{(1-W_i)Y_i(0)}{1-e(X_i)}\\right]\\\\\n",
        "\\mathbb{E}[\\hat{\\tau}^*_{IPW}] &= \\mathbb{E}\\left[\\mathbb{E}\\left[\\frac{W_i Y_i(1)}{e(X_i)}|X_i\\right] - \\mathbb{E}\\left[\\frac{(1-W_i)Y_i(0)}{1-e(X_i)}|X_i\\right]\\right]\\\\\n",
        "\\mathbb{E}[\\hat{\\tau}^*_{IPW}] &= \\mathbb{E}\\left[\\frac{\\mathbb{E}\\left[W_i Y_i(1)|X_i\\right]}{e(X_i)} - \\frac{\\mathbb{E}\\left[(1-W_i) Y_i(0)|X_i\\right]}{1-e(X_i)}\\right]\\\\\n",
        "\\mathbb{E}[\\hat{\\tau}^*_{IPW}] &= \\mathbb{E}\\left[\\frac{\\mathbb{E}\\left[W_i|X_i\\right] \\mathbb{E}\\left[Y_i(1)|X_i\\right]}{e(X_i)} - \\frac{\\mathbb{E}\\left[(1-W_i)|X_i\\right] \\mathbb{E}\\left[Y_i(0)|X_i\\right]}{1-e(X_i)}\\right]\\\\\n",
        "\\mathbb{E}[\\hat{\\tau}^*_{IPW}] &= \\mathbb{E}\\left[\\frac{\\mathbb{P}\\left[W_i|X_i\\right] \\mathbb{E}\\left[Y_i(1)|X_i\\right]}{e(X_i)} - \\frac{\\mathbb{P}\\left[(1-W_i)|X_i\\right] \\mathbb{E}\\left[Y_i(0)|X_i\\right]}{1-e(X_i)}\\right]\\\\\n",
        "\\mathbb{E}[\\hat{\\tau}^*_{IPW}] &= \\mathbb{E}\\left[\\frac{e(X_i) \\mathbb{E}\\left[Y_i(1)|X_i\\right]}{e(X_i)} - \\frac{(1-e(X_i)) \\mathbb{E}\\left[Y_i(0)|X_i\\right]}{1-e(X_i)}\\right]\\\\\n",
        "\\mathbb{E}[\\hat{\\tau}^*_{IPW}] &= \\mathbb{E}\\left[\\mathbb{E}\\left[Y_i(1) - Y_i(0)|X_i\\right]\\right]\\\\\n",
        "\\mathbb{E}[\\hat{\\tau}^*_{IPW}] &= \\tau\\\\\n",
        "\\end{align*}\n",
        "\n",
        "Under unconfoundedness (and SUTVA), $\\hat{\\tau}^*_{IPW}$ is unbiased for $\\tau$. Moreover, because $\\hat{\\tau}^*_{IPW}$ is the sum of $iid$ scores $\\Gamma_{i, IPW^*}$, we can directly apply the Central Limit Theorem to obtain the limiting distribution:\n",
        "\n",
        "$$\\sqrt{n}(\\hat{\\tau}^*_{IPW} - \\tau) \\implies \\mathcal{N}(0, \\mathbb{V}_{IPW*}), \\qquad \\mathbb{V}_{IPW^*} = \\mathbb{V}[\\tau(X_i)] +\\mathbb{E}\\left[\\frac{\\sigma^2_1(X_i)}{e(X_i)} + \\frac{\\sigma^2_0(X_i)}{1-e(X_i)}\\right] + \\mathbb{E}\\left[\\frac{\\left[\\mu_0(X_i)+(1-e(X_i))\\tau(X_i)\\right]^2}{e(X_i)(1-e(X_i))}\\right]$$\n",
        "\n",
        "where $\\mu_w(X_i)$ is the part of the potential outcome $Y_i(w)$ that can be explained with $X_i$, that is, $Y_i(w) = \\mu_w(X_i) + \\varepsilon_i(w)$ with $\\varepsilon_i(w)$ being the ($iid$) random component."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b28b6419",
      "metadata": {},
      "source": [
        "Essentially, IPW is rebalancing our units by assigning more weight to those units whose treatment status is more \"rare\": among the units that received treatment, those with a lower propensity score (and hence with a lower chance of receiving treatment) will have a higher weight. This process attempts to undo the selection in the DGP.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a6a9ac0",
      "metadata": {},
      "source": [
        "One important thing to note here is that IPW makes gives us a simple valid way to estimate treatment effects under unconfoundedness, but we pay a price in terms of variance. In a setting where $\\hat{\\tau}_{STRAT}$ is well-defined, it is straight-forward to see that:\n",
        "\n",
        "$$V_{IPW^*} = V_{STRAT} + \\mathbb{E}\\left[\\frac{\\left[\\mu_0(X_i)+(1-e(X_i))\\tau(X_i)\\right]^2}{e(X_i)(1-e(X_i))}\\right]$$\n",
        "\n",
        "so IPW actually performs worse than the stratified-estimator in such settings.\n",
        "\n",
        "Another drawback of IPW is that it is very sensitive to extreme values of the propensity score. In fact, if for some value of $X_i$ the propensity score is $0$ or $1$, IPW can't be defined. We typically address this problem by requiring the propensity score to not have such extreme values, invoking what we refer to as an \"overlap\" assumption. That is, we assume that there exists an $\\eta >0$ such that: \n",
        "\n",
        "$$\\eta \\leq e(x) \\leq 1-\\eta \\text{ for all } x \\in \\mathcal{X}$$\n",
        "\n",
        "This is called the \"strong overlap\" assumption and it will have implications in how we use IPW in practice. There are some estimators similar to IPW that require a weaker overlap assumption (referred to as \"weak overlap\"). Assuming strong overlap is fine for our purposes so whenever we think of \"overlap\", we will be referring to the strong overlap assumption. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d744db0",
      "metadata": {},
      "source": [
        "Let's see how this all works. Let's introduce a modification to the DGP from our previous exercise that lifts the RCT assumption.\n",
        "\n",
        "\\begin{align*}\n",
        "i &= 1,\\ldots, n\\\\\n",
        "X_{ij} &\\overset{iid}{\\sim} \\mathcal{U}(-1, 1), \\; j \\in \\{1,2,3\\}\\\\\n",
        "e(X_i) &= 0.1 + 0.85 * \\sqrt{\\frac{\\text{max}\\{0,1+X_{2i}+X_{3i}\\}}{3}}\\\\\n",
        "W_i &\\sim Ber(e(X_i))\\\\\n",
        "Y_i &= \\text{exp}(X_{1i} + X_{2i}) + \\text{max}\\{0,X_{3i}\\} W_i  \n",
        "\\end{align*}\n",
        "\n",
        "The average treatment effect in this setting is the same as before:\n",
        "\n",
        "\\begin{align*}\n",
        "\\tau &= \\mathbb{E}[Y_i(1) - Y_i(0)]\\\\\n",
        "\\tau &= \\mathbb{E}[\\text{exp}(X_{1i} + X_{2i}) + \\text{max}\\{0,X_{3i}\\} - \\text{exp}(X_{1i} + X_{2i})]\\\\\n",
        "\\tau &= \\mathbb{E}[\\text{max}\\{0,X_{3i}\\}]\\\\\n",
        "\\tau &= \\mathbb{E}[\\text{max}\\{0,X_{3i}\\}|X_{3i} \\geq 0]\\mathbb{P}[X_{3i}\\geq 0] + \\mathbb{E}[\\text{max}\\{0,X_{3i}\\}|X_{3i}<0]\\mathbb{P}[X_{3i}<0]\\\\\n",
        "\\tau &= \\underbrace{\\mathbb{E}[\\text{max}\\{0,X_{3i}\\}|X_{3i} \\geq 0]}_{0.5}\\underbrace{\\mathbb{P}[X_{3i}\\geq 0]}_{0.5} + 0\\\\\n",
        "\\tau &= 0.25\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84c3fa30",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Disable scientific notation in notebook output\n",
        "pd.options.display.float_format = \"{:.6f}\".format\n",
        "np.set_printoptions(suppress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "44ea0d7e",
      "metadata": {},
      "outputs": [],
      "source": [
        "n = 100_000  # sample size\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# X_ij ~ U(-1, 1) iid for j in {1, 2, 3}\n",
        "X = np.random.uniform(-1, 1, size=(n, 3))\n",
        "\n",
        "# e(X_i) = 0.1 + 0.85 * sqrt(max{0, 1 + X_2i + X_3i} / 3)\n",
        "e_X = 0.1 + 0.85 * np.sqrt(np.maximum(0, 1 + X[:, 1] + X[:, 2]) / 3)\n",
        "\n",
        "# W_i ~ Ber(e(X_i))\n",
        "W = np.random.binomial(1, e_X, size=n)\n",
        "\n",
        "# Y_i = exp(X_1i + X_2i) + max{0, X_3i} * W_i\n",
        "Y = np.exp(X[:, 0] + X[:, 1]) + np.maximum(0, X[:, 2]) * W\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"Y\": Y,\n",
        "    \"W\": W,\n",
        "    \"X1\": X[:, 0],\n",
        "    \"X2\": X[:, 1],\n",
        "    \"X3\": X[:, 2],\n",
        "    \"prop\": e_X\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f59ddf5d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Y</th>\n",
              "      <th>W</th>\n",
              "      <th>X1</th>\n",
              "      <th>X2</th>\n",
              "      <th>X3</th>\n",
              "      <th>prop</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.380504</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.250920</td>\n",
              "      <td>0.901429</td>\n",
              "      <td>0.463988</td>\n",
              "      <td>0.854766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.612231</td>\n",
              "      <td>0</td>\n",
              "      <td>0.197317</td>\n",
              "      <td>-0.687963</td>\n",
              "      <td>-0.688011</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.061665</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.883833</td>\n",
              "      <td>0.732352</td>\n",
              "      <td>0.202230</td>\n",
              "      <td>0.782577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.521005</td>\n",
              "      <td>1</td>\n",
              "      <td>0.416145</td>\n",
              "      <td>-0.958831</td>\n",
              "      <td>0.939820</td>\n",
              "      <td>0.586060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.093697</td>\n",
              "      <td>0</td>\n",
              "      <td>0.664885</td>\n",
              "      <td>-0.575322</td>\n",
              "      <td>-0.636350</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Y  W        X1        X2        X3     prop\n",
              "0 2.380504  1 -0.250920  0.901429  0.463988 0.854766\n",
              "1 0.612231  0  0.197317 -0.687963 -0.688011 0.100000\n",
              "2 1.061665  1 -0.883833  0.732352  0.202230 0.782577\n",
              "3 1.521005  1  0.416145 -0.958831  0.939820 0.586060\n",
              "4 1.093697  0  0.664885 -0.575322 -0.636350 0.100000"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ef08969e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Y</th>\n",
              "      <th>W</th>\n",
              "      <th>X1</th>\n",
              "      <th>X2</th>\n",
              "      <th>X3</th>\n",
              "      <th>prop</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>100000.000000</td>\n",
              "      <td>100000.000000</td>\n",
              "      <td>100000.000000</td>\n",
              "      <td>100000.000000</td>\n",
              "      <td>100000.000000</td>\n",
              "      <td>100000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.563379</td>\n",
              "      <td>0.542160</td>\n",
              "      <td>0.001677</td>\n",
              "      <td>0.000271</td>\n",
              "      <td>0.000937</td>\n",
              "      <td>0.544769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.230819</td>\n",
              "      <td>0.498222</td>\n",
              "      <td>0.576880</td>\n",
              "      <td>0.577664</td>\n",
              "      <td>0.577146</td>\n",
              "      <td>0.230795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.136148</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.999989</td>\n",
              "      <td>-0.999977</td>\n",
              "      <td>-0.999995</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.682901</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.496322</td>\n",
              "      <td>-0.501154</td>\n",
              "      <td>-0.501503</td>\n",
              "      <td>0.415292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.215131</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.002561</td>\n",
              "      <td>0.002313</td>\n",
              "      <td>0.002099</td>\n",
              "      <td>0.590926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2.044180</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.500849</td>\n",
              "      <td>0.499861</td>\n",
              "      <td>0.499962</td>\n",
              "      <td>0.718551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>7.905609</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999984</td>\n",
              "      <td>0.999966</td>\n",
              "      <td>0.999979</td>\n",
              "      <td>0.948683</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Y             W            X1            X2            X3  \\\n",
              "count 100000.000000 100000.000000 100000.000000 100000.000000 100000.000000   \n",
              "mean       1.563379      0.542160      0.001677      0.000271      0.000937   \n",
              "std        1.230819      0.498222      0.576880      0.577664      0.577146   \n",
              "min        0.136148      0.000000     -0.999989     -0.999977     -0.999995   \n",
              "25%        0.682901      0.000000     -0.496322     -0.501154     -0.501503   \n",
              "50%        1.215131      1.000000      0.002561      0.002313      0.002099   \n",
              "75%        2.044180      1.000000      0.500849      0.499861      0.499962   \n",
              "max        7.905609      1.000000      0.999984      0.999966      0.999979   \n",
              "\n",
              "               prop  \n",
              "count 100000.000000  \n",
              "mean       0.544769  \n",
              "std        0.230795  \n",
              "min        0.100000  \n",
              "25%        0.415292  \n",
              "50%        0.590926  \n",
              "75%        0.718551  \n",
              "max        0.948683  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbee7d8b",
      "metadata": {},
      "source": [
        "What would happen if we tried to estimate $\\tau$ with difference-in-means?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0e8abf21",
      "metadata": {},
      "outputs": [],
      "source": [
        "def difference_in_means(df, treatment=\"W\", outcome=\"Y\"):\n",
        "    \"\"\"τ̂_DM = mean(Y | W=1) - mean(Y | W=0).\"\"\"\n",
        "    mean_treated = df.loc[df[treatment] == 1, outcome].mean()\n",
        "    mean_control = df.loc[df[treatment] == 0, outcome].mean()\n",
        "    return mean_treated - mean_control"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a6ac70b3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(0.8131573250669855)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "difference_in_means(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5a9be75",
      "metadata": {},
      "source": [
        "What about linear regression, or Lin's estimator, where we control linearly for $X_i$?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "84c57955",
      "metadata": {},
      "outputs": [],
      "source": [
        "def treatment_effect_ols(df, covariates=None, treatment=\"W\", outcome=\"Y\"):\n",
        "    \"\"\"Estimate treatment effect via OLS: Y ~ intercept + W + covariates.\n",
        "    covariates: list of column names to include as controls.\n",
        "    Returns (estimate, standard_error) for the treatment coefficient.\"\"\"\n",
        "    if covariates is None:\n",
        "        cols = [treatment]\n",
        "    else:    \n",
        "        cols = [treatment] + covariates\n",
        "    X = sm.add_constant(df[cols])\n",
        "    model = sm.OLS(df[outcome], X).fit()\n",
        "    return model.params[treatment], model.bse[treatment]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e4725293",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(np.float64(0.21383950730032236), np.float64(0.003860921029678429))"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "treatment_effect_ols(df, covariates=['X1','X2','X3'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ab07c564",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(np.float64(0.8096154613777504), np.float64(0.005656640056295017))"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "treatment_effect_ols(df, covariates=['X1'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "350256d0",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(np.float64(0.2196216973416687), np.float64(0.006584333120974501))"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "treatment_effect_ols(df, covariates=['X2','X3'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7695263b",
      "metadata": {},
      "source": [
        "Looks better, but bias is ~10% of the estimator. Let's try the two new estimators we proposed above, starting with $\\hat{\\tau}_{IPW^*}$. Note that because CLT holds, we can simply estimate the asymptotic variance with the sample variance of the scores $\\Gamma_{i, IPW^*}$.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "aa49150a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def ipw_ate(df, propensity_col, y_col=\"Y\", w_col=\"W\"):\n",
        "    \"\"\"\n",
        "    Estimate the average treatment effect (ATE) using inverse probability weighting.\n",
        "\n",
        "    τ̂_IPW = (1/n) Σ [ W_i * Y_i / e(X_i) - (1 - W_i) * Y_i / (1 - e(X_i)) ]\n",
        "\n",
        "    Standard error is computed from the sample variance of the IPW score (influence) terms.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        DataFrame with outcome, treatment, and propensity score.\n",
        "    propensity_col : str\n",
        "        Name of the column containing the propensity score e(X) = P(W=1|X).\n",
        "    y_col : str, default \"Y\"\n",
        "        Name of the outcome column.\n",
        "    w_col : str, default \"W\"\n",
        "        Name of the treatment indicator column (0/1).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple of (float, float)\n",
        "        (tau_hat, standard_error).\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If any propensity score is not in (0, 1) (overlap does not hold).\n",
        "    \"\"\"\n",
        "    e = df[propensity_col].values\n",
        "    if np.any(e <= 0) or np.any(e >= 1):\n",
        "        raise ValueError(\n",
        "            \"Overlap does not hold: all propensity scores must be strictly in (0, 1). \"\n",
        "            \"Found values outside this range.\"\n",
        "        )\n",
        "    y = df[y_col].values\n",
        "    w = df[w_col].values\n",
        "    n = len(df)\n",
        "    # IPW score: ψ_i = W_i*Y_i/e_i - (1-W_i)*Y_i/(1-e_i)\n",
        "    scores = (w * y / e) - ((1 - w) * y / (1 - e))\n",
        "    tau_hat = np.mean(scores)\n",
        "    se = np.std(scores, ddof=1) / np.sqrt(n)\n",
        "    return float(tau_hat), float(se)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "3b2f7a1a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.24309745978275038, 0.014498603659518557)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ipw_ate(df,propensity_col='prop')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "101989b2",
      "metadata": {},
      "source": [
        "So much better! Let's compare this to $\\hat{\\tau}_{STRAT}$ using the propensity score as stratification variable and 20 strata.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "996ba22f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def stratified_ate(\n",
        "    df,\n",
        "    propensity_col,\n",
        "    strata_bounds,\n",
        "    y_col=\"Y\",\n",
        "    w_col=\"W\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Estimate the ATE using the propensity-score stratified estimator.\n",
        "\n",
        "    Units are binned by propensity score using strata_bounds; within each stratum\n",
        "    the conditional ATE is estimated by difference-in-means, then aggregated\n",
        "    with weights n_x / n.\n",
        "\n",
        "    τ̂_STRAT = Σ_x (n_x / n) * τ̂(x),   τ̂(x) = E[Y|W=1,X∈stratum x] - E[Y|W=0,X∈stratum x]\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        DataFrame with outcome, treatment, and propensity score.\n",
        "    propensity_col : str\n",
        "        Name of the column containing the propensity score e(X) = P(W=1|X).\n",
        "    strata_bounds : int or array-like\n",
        "        If int: number of strata; bounds are np.linspace(0, 1, strata_bounds + 1).\n",
        "        If array-like: 1d array of bin edges (length = number of strata + 1).\n",
        "    y_col : str, default \"Y\"\n",
        "        Name of the outcome column.\n",
        "    w_col : str, default \"W\"\n",
        "        Name of the treatment indicator column (0/1).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        Stratified estimate of the ATE.\n",
        "    \"\"\"\n",
        "    e = df[propensity_col].values\n",
        "    y = df[y_col].values\n",
        "    w = df[w_col].values\n",
        "    n = len(df)\n",
        "\n",
        "    if np.isscalar(strata_bounds) and isinstance(strata_bounds, (int, np.integer)):\n",
        "        bounds = np.linspace(0.0, 1.0, int(strata_bounds) + 1)\n",
        "    else:\n",
        "        bounds = np.asarray(strata_bounds, dtype=float)\n",
        "    if bounds.ndim != 1 or len(bounds) < 2:\n",
        "        raise ValueError(\"strata_bounds must be an int or a 1d array of at least 2 edges.\")\n",
        "\n",
        "    # Assign stratum: (bounds[0], bounds[1]], (bounds[1], bounds[2]], ..., (bounds[-2], bounds[-1]]\n",
        "    stratum = np.digitize(e, bounds, right=False) - 1\n",
        "    # clip so e exactly equal to bounds[-1] stays in last stratum\n",
        "    stratum = np.clip(stratum, 0, len(bounds) - 2)\n",
        "\n",
        "    tau_hat = 0.0\n",
        "    for s in range(len(bounds) - 1):\n",
        "        mask = stratum == s\n",
        "        n_x = np.sum(mask)\n",
        "        if n_x == 0:\n",
        "            continue\n",
        "        w1 = mask & (w == 1)\n",
        "        w0 = mask & (w == 0)\n",
        "        n_x1 = np.sum(w1)\n",
        "        n_x0 = np.sum(w0)\n",
        "        if n_x1 == 0 or n_x0 == 0:\n",
        "            raise ValueError(\n",
        "                f\"Overlap does not hold in stratum {s} (e in ({bounds[s]:.4f}, {bounds[s+1]:.4f}]): \"\n",
        "                \"need both treated and control units.\"\n",
        "            )\n",
        "        mean1 = np.sum(np.where(w1, y, 0.0)) / n_x1\n",
        "        mean0 = np.sum(np.where(w0, y, 0.0)) / n_x0\n",
        "        tau_x = mean1 - mean0\n",
        "        tau_hat += (n_x / n) * tau_x\n",
        "\n",
        "    return float(tau_hat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "66270c95",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.2571479247938724"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stratified_ate(\n",
        "    df,\n",
        "    propensity_col='prop',\n",
        "    strata_bounds=20,\n",
        "    y_col=\"Y\",\n",
        "    w_col=\"W\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d45f50f",
      "metadata": {},
      "source": [
        "## Bootstrap"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78f73d5d",
      "metadata": {},
      "source": [
        "Unlike the estimators we have computed so far, there is no immediate way to compute the variance of $\\hat{\\tau}_{STRAT}$ with a plugin estimator for $V_{STRAT}$. There is, however, a statistical technique that allows us to compute the distribution of almost any estimator through resampling methods: bootstrapping.\n",
        "\n",
        "Bootstrapping is as conceptually simple as it is powerful. Given an estimator we can compute with a sample, bootstrapping allows to estimate the sampling distribution of the estimator by constructing $B$ new samples through resampling the original data with replacement $B$ times. In each of those new samples, we compute our estimator creating a distribution with the estimates. We then use that distribution to compute different statistics: mean, variance, confidence intervals, etc. Bootstrap. works almost everywhere assuming the observations are $iid$ and the sample is representative from the population we want to make inference of.\n",
        "\n",
        "We want to use bootstrap to obtain an estimate of the variance and confidence intervals for $\\hat{\\tau}_{STRAT}$. There are a number of ways to do it, but we will obtain each using the most common method. For the variance, we will compute the sample variance of the estimator across our bootstrapped samples. For the (95%) confidence interval, we will grab the 2.5th and 97.5th percentiles of the bootstrapped distribution. Because we know the limiting distribution of $\\hat{\\tau}_{STRAT}$ (Normal), we will compare the percentile method of constructing the confidence intervals to simply using the bootstrapped variance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aca3614b",
      "metadata": {},
      "source": [
        "Let's first evaluate whether bootstrap provides a reasonable estimate of the variance in the cases we can directly compute its expression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "aa721db0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _extract_estimate(result):\n",
        "    \"\"\"If estimator returns (estimate, ...), return estimate; else return result.\"\"\"\n",
        "    if isinstance(result, (tuple, list)):\n",
        "        return result[0]\n",
        "    return result\n",
        "\n",
        "def bootstrap_ate(df, estimator, B=1000, seed=None, **estimator_kwargs):\n",
        "    \"\"\"\n",
        "    Estimate the sampling distribution of an ATE estimator via bootstrap.\n",
        "\n",
        "    Resamples the data with replacement B times, runs the estimator on each\n",
        "    resample, then reports the point estimate (from the original sample),\n",
        "    bootstrap standard error (sample std of the B estimates), and a 95% percentile\n",
        "    confidence interval.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Original sample (will be resampled with replacement).\n",
        "    estimator : callable\n",
        "        Function that takes a DataFrame and optional keyword arguments and\n",
        "        returns either a float (the ATE estimate) or a tuple/list whose first\n",
        "        element is the estimate (e.g. (tau_hat, se)). Examples: difference_in_means,\n",
        "        treatment_effect_ols, ipw_ate, stratified_ate.\n",
        "    B : int, default 1000\n",
        "        Number of bootstrap replications.\n",
        "    seed : int or None, default None\n",
        "        Random seed for resampling.\n",
        "    **estimator_kwargs\n",
        "        Keyword arguments passed to estimator on each call (e.g. propensity_col,\n",
        "        strata_bounds, covariates).\n",
        "    -------\n",
        "    float\n",
        "        Stratified estimate of the ATE.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n = len(df)\n",
        "\n",
        "    point_estimate = _extract_estimate(estimator(df, **estimator_kwargs))\n",
        "    estimates = []\n",
        "    for _ in range(B):\n",
        "        idx = rng.integers(0, n, size=n)\n",
        "        df_b = df.iloc[idx].reset_index(drop=True)\n",
        "        result_b = estimator(df_b, **estimator_kwargs)\n",
        "        estimates.append(_extract_estimate(result_b))\n",
        "    estimates = np.array(estimates)\n",
        "\n",
        "    mean = np.mean(estimates)\n",
        "    se = np.std(estimates, ddof=1)\n",
        "    ci_percentile = (float(np.percentile(estimates, 2.5)), float(np.percentile(estimates, 97.5)))\n",
        "    z = 1.96  # 95% two-sided\n",
        "    ci_normal = (float(point_estimate - z * se), float(point_estimate + z * se))\n",
        "    return float(point_estimate), float(mean), float(se), ci_percentile, ci_normal, estimates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "e2823097",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(np.float64(0.21383950730032236), np.float64(0.003860921029678429))"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "treatment_effect_ols(df, covariates=['X1','X2','X3'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "725cbba1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.21383950730032236,\n",
              " 0.21370524127984727,\n",
              " 0.0038210004036056977,\n",
              " (0.20597291223017886, 0.220809119044536),\n",
              " (0.20635034650925518, 0.22132866809138954),\n",
              " array([0.20276532, 0.2157657 , 0.20774868, 0.21259165, 0.21203783,\n",
              "        0.21629915, 0.20802821, 0.22056523, 0.21056767, 0.21080219,\n",
              "        0.21360476, 0.21115806, 0.21337136, 0.21559252, 0.21184404,\n",
              "        0.21242526, 0.21364655, 0.21439071, 0.21369793, 0.20382136,\n",
              "        0.21313221, 0.21317881, 0.21686886, 0.20726322, 0.21548194,\n",
              "        0.20648373, 0.21658129, 0.21127431, 0.21834083, 0.20407162,\n",
              "        0.21097817, 0.21411484, 0.20813151, 0.21330494, 0.21153199,\n",
              "        0.21537956, 0.22144512, 0.21080377, 0.2092344 , 0.21602655,\n",
              "        0.20767739, 0.21324871, 0.21105459, 0.21263625, 0.21126876,\n",
              "        0.21673528, 0.22081516, 0.21623256, 0.21139672, 0.21337237,\n",
              "        0.21707097, 0.21437123, 0.21380814, 0.21421751, 0.21177051,\n",
              "        0.21752843, 0.21426536, 0.21926954, 0.21190545, 0.21261253,\n",
              "        0.21884095, 0.21525778, 0.20673911, 0.21449003, 0.21687044,\n",
              "        0.21523538, 0.21263726, 0.20984309, 0.20977776, 0.22014678,\n",
              "        0.22009921, 0.21168173, 0.21225715, 0.21866483, 0.20953415,\n",
              "        0.21499478, 0.21246421, 0.20922717, 0.21717422, 0.21640069,\n",
              "        0.20801672, 0.21669999, 0.20778227, 0.21419159, 0.20691845,\n",
              "        0.214905  , 0.21407449, 0.2171391 , 0.21609505, 0.21030386,\n",
              "        0.21105251, 0.21001012, 0.21665045, 0.21247742, 0.21883175,\n",
              "        0.21575933, 0.21331159, 0.21831148, 0.21148313, 0.21364033,\n",
              "        0.21131908, 0.21228424, 0.20503034, 0.21244672, 0.2152136 ,\n",
              "        0.20908813, 0.21689106, 0.21175489, 0.21663851, 0.21260378,\n",
              "        0.20925662, 0.21412337, 0.21589153, 0.20902668, 0.21518589,\n",
              "        0.21534734, 0.21824886, 0.21210884, 0.21986258, 0.21521571,\n",
              "        0.21192119, 0.21787268, 0.21434829, 0.21530237, 0.21461621,\n",
              "        0.21037305, 0.21108485, 0.21508677, 0.21772344, 0.21357685,\n",
              "        0.21604246, 0.21229867, 0.21428038, 0.21436382, 0.22183285,\n",
              "        0.21690479, 0.21719898, 0.21808643, 0.21437333, 0.20696175,\n",
              "        0.21847156, 0.22066334, 0.20829483, 0.2124597 , 0.21005264,\n",
              "        0.2144228 , 0.21079921, 0.21661909, 0.2185044 , 0.2119617 ,\n",
              "        0.21069053, 0.21681059, 0.21868799, 0.21181163, 0.20549569,\n",
              "        0.21474913, 0.21292308, 0.21741077, 0.21144118, 0.21345863,\n",
              "        0.21343547, 0.21364869, 0.21416436, 0.2184583 , 0.21561431,\n",
              "        0.20734354, 0.21697903, 0.21512835, 0.21319915, 0.21674931,\n",
              "        0.21016731, 0.21295993, 0.21688   , 0.21340311, 0.21071019,\n",
              "        0.21415647, 0.21051106, 0.21860568, 0.21175641, 0.21768807,\n",
              "        0.2149268 , 0.21766287, 0.21589173, 0.21014904, 0.21598705,\n",
              "        0.21673625, 0.21166377, 0.20734997, 0.21027562, 0.21498966,\n",
              "        0.21637883, 0.21380319, 0.21592347, 0.21703888, 0.20574605,\n",
              "        0.21190053, 0.21385015, 0.2057901 , 0.21770178, 0.21481055,\n",
              "        0.208552  , 0.22244797, 0.220086  , 0.21387555, 0.22218518,\n",
              "        0.20845083, 0.21851952, 0.21303664, 0.22166694, 0.21598214,\n",
              "        0.21469577, 0.21246257, 0.21406029, 0.21188088, 0.21474375,\n",
              "        0.20934739, 0.21690737, 0.20660178, 0.215281  , 0.21096361,\n",
              "        0.21313899, 0.20952333, 0.21496961, 0.21701569, 0.21001971,\n",
              "        0.20865853, 0.21709029, 0.21207361, 0.21030051, 0.21311361,\n",
              "        0.20843025, 0.2108189 , 0.20530163, 0.21237361, 0.22294087,\n",
              "        0.21055191, 0.219214  , 0.21025433, 0.20781958, 0.21409708,\n",
              "        0.21149236, 0.21931655, 0.20840507, 0.2101014 , 0.21719774,\n",
              "        0.21346599, 0.21245054, 0.2098412 , 0.21193417, 0.21435996,\n",
              "        0.21132658, 0.21483227, 0.21498446, 0.21274414, 0.20306128,\n",
              "        0.21783053, 0.21065675, 0.21227198, 0.20825585, 0.21671501,\n",
              "        0.21447145, 0.22350642, 0.217251  , 0.21572794, 0.21837044,\n",
              "        0.21099022, 0.21206735, 0.21239421, 0.21090898, 0.21589026,\n",
              "        0.2033232 , 0.21522986, 0.21652184, 0.20706717, 0.21796683,\n",
              "        0.20835399, 0.21242225, 0.21534013, 0.21499647, 0.22203954,\n",
              "        0.21727849, 0.21273382, 0.21415183, 0.21281803, 0.22253221,\n",
              "        0.21439507, 0.21890404, 0.21117223, 0.21000902, 0.21278748,\n",
              "        0.21113049, 0.21581367, 0.21286744, 0.21403195, 0.21001773,\n",
              "        0.20811401, 0.21260566, 0.21019369, 0.21558274, 0.21034958,\n",
              "        0.21272435, 0.21538813, 0.21688572, 0.20326896, 0.2101985 ,\n",
              "        0.21408705, 0.21020118, 0.21515414, 0.2162549 , 0.21244319,\n",
              "        0.21326646, 0.2193291 , 0.21863831, 0.22118903, 0.21470603,\n",
              "        0.21148103, 0.21526625, 0.20803457, 0.21837026, 0.21309286,\n",
              "        0.21509331, 0.2141395 , 0.21385746, 0.21980454, 0.21632902,\n",
              "        0.21656358, 0.20680256, 0.21420365, 0.2147806 , 0.21778388,\n",
              "        0.21319942, 0.2096526 , 0.21972877, 0.21427468, 0.21235083,\n",
              "        0.21479849, 0.21796973, 0.21793502, 0.21201932, 0.21418348,\n",
              "        0.21585299, 0.21764541, 0.21298748, 0.21967331, 0.20821533,\n",
              "        0.2111373 , 0.21413383, 0.21738453, 0.2095403 , 0.2200738 ,\n",
              "        0.2207898 , 0.21623514, 0.21252073, 0.21504256, 0.2132008 ,\n",
              "        0.21858209, 0.21850205, 0.2166838 , 0.21700155, 0.21133951,\n",
              "        0.21439518, 0.21189528, 0.22041395, 0.21344917, 0.2176808 ,\n",
              "        0.21329515, 0.21039894, 0.20891227, 0.21760299, 0.21362565,\n",
              "        0.2256227 , 0.21762339, 0.21336965, 0.21961908, 0.21291648,\n",
              "        0.21600381, 0.21176319, 0.2229787 , 0.21585574, 0.21550795,\n",
              "        0.21692986, 0.21629008, 0.20712431, 0.21601249, 0.21251156,\n",
              "        0.21293323, 0.2126007 , 0.20792347, 0.21324917, 0.2122481 ,\n",
              "        0.21890255, 0.21125604, 0.20719863, 0.22088599, 0.21314228,\n",
              "        0.21729287, 0.21736072, 0.20779686, 0.21105739, 0.21175178,\n",
              "        0.21555612, 0.21194797, 0.21026496, 0.2081412 , 0.21504048,\n",
              "        0.2156891 , 0.21753202, 0.2156175 , 0.21383943, 0.21243611,\n",
              "        0.21352533, 0.21878043, 0.21661043, 0.21644991, 0.21378586,\n",
              "        0.2094165 , 0.21223702, 0.21026876, 0.20467621, 0.22056675,\n",
              "        0.21688661, 0.2143917 , 0.20744957, 0.20308785, 0.21555522,\n",
              "        0.21608773, 0.20614849, 0.21377965, 0.20764029, 0.21065962,\n",
              "        0.20702048, 0.21454343, 0.21168596, 0.21031961, 0.21934367,\n",
              "        0.21826954, 0.21345783, 0.21019475, 0.21570582, 0.21157587,\n",
              "        0.21579827, 0.2133238 , 0.21524985, 0.20780961, 0.21079079,\n",
              "        0.21368146, 0.21177875, 0.21587938, 0.21449166, 0.21136368,\n",
              "        0.21121695, 0.21080294, 0.21658357, 0.21296735, 0.2065806 ,\n",
              "        0.21256516, 0.21270977, 0.2119404 , 0.21228977, 0.21172479,\n",
              "        0.2149527 , 0.21684346, 0.20883617, 0.21392665, 0.21449096,\n",
              "        0.21173826, 0.21364394, 0.21097077, 0.2169475 , 0.21087671,\n",
              "        0.22062651, 0.21582811, 0.20337634, 0.21208661, 0.21277152,\n",
              "        0.21554067, 0.2084358 , 0.20583523, 0.21542526, 0.20729033,\n",
              "        0.21384651, 0.20669814, 0.21889726, 0.21911695, 0.21695985,\n",
              "        0.20992429, 0.21507882, 0.2193425 , 0.2137443 , 0.21195509,\n",
              "        0.21288298, 0.22411612, 0.20987101, 0.21104489, 0.21358285,\n",
              "        0.21456969, 0.20852613, 0.21004829, 0.20963775, 0.20731282,\n",
              "        0.21632201, 0.21117799, 0.20920562, 0.20959542, 0.21447242,\n",
              "        0.20867307, 0.21143675, 0.21344108, 0.21365987, 0.21364794,\n",
              "        0.20851362, 0.21168225, 0.20158736, 0.21864195, 0.21256696,\n",
              "        0.21239246, 0.21291795, 0.21156169, 0.21522135, 0.21662352,\n",
              "        0.21321311, 0.20493231, 0.21093502, 0.20496964, 0.21539258,\n",
              "        0.20760997, 0.20831497, 0.20989298, 0.20903978, 0.21500916,\n",
              "        0.21578799, 0.21039703, 0.21407929, 0.21264768, 0.21558119,\n",
              "        0.21013881, 0.21537396, 0.21740156, 0.21401874, 0.20943523,\n",
              "        0.21718937, 0.20829219, 0.21454684, 0.21099145, 0.21608834,\n",
              "        0.21554301, 0.21453328, 0.21379568, 0.21389871, 0.21561958,\n",
              "        0.21852509, 0.21364009, 0.21219662, 0.21529437, 0.21820628,\n",
              "        0.210653  , 0.20918561, 0.20706558, 0.20952229, 0.21449866,\n",
              "        0.21912136, 0.21602983, 0.21363045, 0.21545979, 0.21877841,\n",
              "        0.21805497, 0.20953597, 0.21652418, 0.2125494 , 0.2139899 ,\n",
              "        0.2159896 , 0.21570591, 0.21158106, 0.21258778, 0.21015414,\n",
              "        0.21287738, 0.2138524 , 0.21405105, 0.21321561, 0.21957459,\n",
              "        0.20819368, 0.21613497, 0.22055889, 0.21489248, 0.21263194,\n",
              "        0.22064746, 0.21641836, 0.21503722, 0.2157715 , 0.21451132,\n",
              "        0.21935796, 0.21414631, 0.22506821, 0.21025516, 0.21416886,\n",
              "        0.20709106, 0.21839439, 0.21194198, 0.21101338, 0.21146252,\n",
              "        0.21542089, 0.21226931, 0.22036518, 0.21332896, 0.21967042,\n",
              "        0.21358241, 0.21305268, 0.2255702 , 0.21716683, 0.21548457,\n",
              "        0.2166099 , 0.21569531, 0.22448498, 0.21189566, 0.21218923,\n",
              "        0.21375414, 0.21470247, 0.21548518, 0.21692046, 0.21501864,\n",
              "        0.2090618 , 0.21844473, 0.21441269, 0.21254342, 0.20998085,\n",
              "        0.21930771, 0.21166059, 0.22216766, 0.21139885, 0.21467703,\n",
              "        0.21519848, 0.20987049, 0.21395956, 0.21018061, 0.21752952,\n",
              "        0.21070153, 0.21500718, 0.21294636, 0.2182605 , 0.21066441,\n",
              "        0.21420007, 0.21549809, 0.21251449, 0.2059715 , 0.2117694 ,\n",
              "        0.21729231, 0.20836516, 0.2113131 , 0.21556143, 0.21163226,\n",
              "        0.20714803, 0.22300975, 0.21667468, 0.21405091, 0.21655558,\n",
              "        0.21640198, 0.20710438, 0.21557409, 0.21280659, 0.20983591,\n",
              "        0.21564138, 0.21715937, 0.21472517, 0.21834005, 0.21324786,\n",
              "        0.2115502 , 0.21288671, 0.20829373, 0.21989546, 0.21121337,\n",
              "        0.21684877, 0.21309154, 0.21115648, 0.21254889, 0.20491357,\n",
              "        0.21204114, 0.21389603, 0.21091214, 0.21674897, 0.20778673,\n",
              "        0.20849991, 0.21629455, 0.21548922, 0.21439973, 0.21338249,\n",
              "        0.21335291, 0.21052889, 0.20982342, 0.21341447, 0.21299129,\n",
              "        0.20874811, 0.2097207 , 0.21309953, 0.20924597, 0.21109358,\n",
              "        0.21673844, 0.20681916, 0.21748017, 0.21809179, 0.21375552,\n",
              "        0.21423189, 0.21183396, 0.21070899, 0.22116649, 0.219845  ,\n",
              "        0.2093494 , 0.20839378, 0.2089901 , 0.21460116, 0.2192645 ,\n",
              "        0.20753439, 0.21251615, 0.2163301 , 0.21702685, 0.21491442,\n",
              "        0.21476831, 0.2132744 , 0.21791327, 0.21132451, 0.21550415,\n",
              "        0.20972247, 0.20760734, 0.21583748, 0.21367244, 0.2195928 ,\n",
              "        0.21512974, 0.20646725, 0.22080896, 0.21912218, 0.21898498,\n",
              "        0.21390073, 0.21432576, 0.2130616 , 0.20324225, 0.21401995,\n",
              "        0.21129482, 0.21424025, 0.20844001, 0.21640674, 0.21398992,\n",
              "        0.21017318, 0.21979012, 0.21604773, 0.21750087, 0.21021198,\n",
              "        0.21761369, 0.21979898, 0.21748705, 0.21141299, 0.21682315,\n",
              "        0.2140632 , 0.21585385, 0.21682002, 0.21377768, 0.21743068,\n",
              "        0.21821872, 0.2101556 , 0.20956432, 0.21574989, 0.20775991,\n",
              "        0.21541288, 0.21556971, 0.21393807, 0.21234918, 0.2105983 ,\n",
              "        0.219728  , 0.21592475, 0.21106313, 0.22054749, 0.20951251,\n",
              "        0.21481508, 0.21025234, 0.21369226, 0.21787269, 0.21503555,\n",
              "        0.21058304, 0.22046985, 0.209022  , 0.22036302, 0.21550299,\n",
              "        0.21031498, 0.20597295, 0.21877216, 0.21725859, 0.20996763,\n",
              "        0.21850893, 0.21771749, 0.21801543, 0.21497287, 0.21635917,\n",
              "        0.21626179, 0.21624609, 0.21026654, 0.21364048, 0.20458853,\n",
              "        0.21832808, 0.21267563, 0.21513493, 0.21139757, 0.21497341,\n",
              "        0.21647103, 0.21249923, 0.21310706, 0.21035916, 0.21138859,\n",
              "        0.21659302, 0.21595394, 0.2130214 , 0.21545161, 0.2131995 ,\n",
              "        0.22057295, 0.22048259, 0.21601391, 0.2119922 , 0.21625369,\n",
              "        0.21403961, 0.2134205 , 0.22288769, 0.2137328 , 0.20947907,\n",
              "        0.21092055, 0.21434274, 0.21130522, 0.20600148, 0.21600744,\n",
              "        0.21488312, 0.21309502, 0.20381783, 0.2190347 , 0.21254491,\n",
              "        0.2118664 , 0.21017512, 0.21891213, 0.20937995, 0.20762412,\n",
              "        0.2157066 , 0.21483263, 0.21951233, 0.21194248, 0.21515632,\n",
              "        0.21370238, 0.21652583, 0.21525824, 0.20943541, 0.21053805,\n",
              "        0.21818254, 0.21364851, 0.21491433, 0.21330886, 0.21089178,\n",
              "        0.21884741, 0.21372853, 0.2166346 , 0.21385783, 0.21386513,\n",
              "        0.21553398, 0.21558574, 0.20699316, 0.21835828, 0.20941392,\n",
              "        0.21027659, 0.21372116, 0.21451129, 0.21371284, 0.21215385,\n",
              "        0.21173067, 0.21597126, 0.21246278, 0.20988993, 0.21385124,\n",
              "        0.22045802, 0.21402998, 0.21107349, 0.2124971 , 0.21387763,\n",
              "        0.21676723, 0.21924514, 0.21003867, 0.21002074, 0.21353897,\n",
              "        0.21528737, 0.21823052, 0.21676404, 0.20825439, 0.21037286,\n",
              "        0.21786534, 0.21356546, 0.2129264 , 0.21596248, 0.21575068,\n",
              "        0.21491015, 0.21781091, 0.20785116, 0.21548565, 0.21427618,\n",
              "        0.21466862, 0.21791502, 0.21319927, 0.22154495, 0.20155801,\n",
              "        0.21263327, 0.21643108, 0.21855503, 0.21561207, 0.21207739,\n",
              "        0.21539199, 0.22150297, 0.21594564, 0.21308688, 0.21491894,\n",
              "        0.20938481, 0.21341249, 0.21862084, 0.21208108, 0.21723529,\n",
              "        0.20780916, 0.21261939, 0.21738464, 0.21335005, 0.21586858,\n",
              "        0.21631669, 0.20890432, 0.2097004 , 0.21126919, 0.2123744 ,\n",
              "        0.21838843, 0.21843907, 0.2157042 , 0.21259554, 0.21121764,\n",
              "        0.21380169, 0.21448966, 0.21654258, 0.21763467, 0.21342611,\n",
              "        0.21387947, 0.21597351, 0.21182959, 0.21291427, 0.21602107,\n",
              "        0.21152821, 0.21362771, 0.21674836, 0.21386811, 0.2156006 ,\n",
              "        0.2130284 , 0.21244148, 0.21852108, 0.21602565, 0.21702205,\n",
              "        0.21122505, 0.21561691, 0.21335481, 0.21574605, 0.2202588 ,\n",
              "        0.2148863 , 0.21233305, 0.21655026, 0.21244063, 0.21259817,\n",
              "        0.21355991, 0.2141635 , 0.21357931, 0.21668829, 0.22258598,\n",
              "        0.21385641, 0.21503785, 0.21019489, 0.20888677, 0.21003234,\n",
              "        0.2076827 , 0.20306251, 0.21194433, 0.20809011, 0.21147049,\n",
              "        0.21226531, 0.20772946, 0.21962998, 0.21022863, 0.21374421,\n",
              "        0.20940963, 0.21330053, 0.21014415, 0.21061127, 0.20732994,\n",
              "        0.21923828, 0.2095239 , 0.21270395, 0.21740805, 0.21548265,\n",
              "        0.21405375, 0.21600858, 0.2158242 , 0.21679875, 0.20979218,\n",
              "        0.21710072, 0.21369203, 0.21288574, 0.21565038, 0.21206134]))"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bootstrap_ate(df, estimator=treatment_effect_ols, B=1000, seed=None, covariates=['X1','X2','X3'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80da6851",
      "metadata": {},
      "source": [
        "The bootstrapped variance and CI look pretty close to the ones obtained with the plugin estimators, suggesting that the bootstrap technique can provide a way to obtain the sampling distribution. Let's now use this learning and compute the variance and CI for $\\hat{\\tau}_{STRAT}$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79bad1d2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.24309745978275038, 0.014498603659518557)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ipw_ate(df,propensity_col='prop')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "86f48cdc",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.2571479247938724,\n",
              " 0.2572845673334307,\n",
              " 0.007810963078895144,\n",
              " (0.24280496187018416, 0.27225022796633),\n",
              " (0.24183843715923792, 0.27245741242850685),\n",
              " array([0.2387051 , 0.25753081, 0.25755214, 0.25019727, 0.24896149,\n",
              "        0.25912717, 0.26014099, 0.25314496, 0.25963582, 0.27373272,\n",
              "        0.25227444, 0.25212386, 0.24880197, 0.26630476, 0.25306689,\n",
              "        0.26940874, 0.24943194, 0.25990998, 0.26221697, 0.26190743,\n",
              "        0.25349889, 0.2475238 , 0.25406984, 0.25843009, 0.25102838,\n",
              "        0.2638045 , 0.24696795, 0.26085693, 0.24475442, 0.26381745,\n",
              "        0.27190637, 0.26406299, 0.25915795, 0.27364279, 0.26539456,\n",
              "        0.26768976, 0.26485678, 0.25986104, 0.26683387, 0.25468731,\n",
              "        0.24220392, 0.25505379, 0.26614312, 0.25115897, 0.27009317,\n",
              "        0.26892014, 0.24972873, 0.26903216, 0.26602706, 0.25702862,\n",
              "        0.26621325, 0.24728706, 0.26948991, 0.26054096, 0.25332558,\n",
              "        0.26780557, 0.25435419, 0.26492021, 0.24491134, 0.23514049,\n",
              "        0.25711145, 0.24842275, 0.24504327, 0.25595132, 0.2642558 ,\n",
              "        0.25207545, 0.25078415, 0.25222124, 0.236178  , 0.26812747,\n",
              "        0.26248793, 0.25237899, 0.26144495, 0.25108461, 0.25800415,\n",
              "        0.25952728, 0.26442218, 0.24957895, 0.25571644, 0.24775871,\n",
              "        0.23995568, 0.25297256, 0.2574103 , 0.25561343, 0.25430856,\n",
              "        0.25835266, 0.25439326, 0.26239921, 0.2552302 , 0.25201682,\n",
              "        0.24932202, 0.26691957, 0.25261772, 0.2568378 , 0.2673269 ,\n",
              "        0.2639178 , 0.2476252 , 0.25559955, 0.26144815, 0.25555038,\n",
              "        0.26616219, 0.26618895, 0.25605639, 0.25706175, 0.24951287,\n",
              "        0.27765346, 0.26438437, 0.24737512, 0.24199093, 0.26046446,\n",
              "        0.2657286 , 0.26458946, 0.25294686, 0.26412593, 0.2577122 ,\n",
              "        0.2485293 , 0.26441017, 0.2556026 , 0.26118757, 0.27224998,\n",
              "        0.25401417, 0.24548922, 0.25397483, 0.25356366, 0.26290421,\n",
              "        0.25088445, 0.25014151, 0.2572831 , 0.25520156, 0.26385217,\n",
              "        0.25848853, 0.25192262, 0.2540719 , 0.26616559, 0.24817679,\n",
              "        0.25554331, 0.26592728, 0.25864231, 0.25598625, 0.25985226,\n",
              "        0.26476725, 0.26118312, 0.2593103 , 0.25330462, 0.26114039,\n",
              "        0.25893292, 0.26311437, 0.25660123, 0.25404901, 0.2633455 ,\n",
              "        0.26419194, 0.28035981, 0.24704149, 0.26298983, 0.26163654,\n",
              "        0.25334292, 0.25940289, 0.25537603, 0.25242793, 0.25239527,\n",
              "        0.27232149, 0.24624162, 0.26316894, 0.27445639, 0.25348448,\n",
              "        0.26243783, 0.26875894, 0.25001381, 0.26232912, 0.26328208,\n",
              "        0.2624119 , 0.26307649, 0.2669109 , 0.26418649, 0.2551692 ,\n",
              "        0.25183548, 0.25640449, 0.24234115, 0.26810427, 0.26610244,\n",
              "        0.24784483, 0.24910089, 0.24214506, 0.25556569, 0.25046465,\n",
              "        0.24524622, 0.25540859, 0.26896936, 0.26826703, 0.26477531,\n",
              "        0.25120159, 0.2558777 , 0.25651188, 0.25286654, 0.26364011,\n",
              "        0.24695336, 0.25377213, 0.24339162, 0.26218858, 0.26001034,\n",
              "        0.25260247, 0.26516266, 0.25683063, 0.24710082, 0.26717657,\n",
              "        0.26141269, 0.25708375, 0.24515343, 0.25550916, 0.25738135,\n",
              "        0.26978284, 0.26952831, 0.25217017, 0.24659946, 0.24314083,\n",
              "        0.24229772, 0.25991236, 0.25618054, 0.25295732, 0.24842531,\n",
              "        0.24793668, 0.25703443, 0.2662991 , 0.25588949, 0.25949958,\n",
              "        0.2477368 , 0.2430964 , 0.26013691, 0.25446255, 0.24861332,\n",
              "        0.24888429, 0.27276109, 0.25351186, 0.26030921, 0.24750366,\n",
              "        0.26006047, 0.25163183, 0.26459202, 0.2498483 , 0.24793822,\n",
              "        0.25661846, 0.26355847, 0.25445519, 0.26824784, 0.25061042,\n",
              "        0.26964863, 0.26642694, 0.25459705, 0.25666769, 0.2647291 ,\n",
              "        0.24945069, 0.25222957, 0.26598345, 0.26439317, 0.26808331,\n",
              "        0.26070787, 0.26035891, 0.25276268, 0.24851024, 0.2741844 ,\n",
              "        0.26243804, 0.25776736, 0.24604909, 0.26880782, 0.25782869,\n",
              "        0.25172801, 0.25509938, 0.25550332, 0.25373971, 0.25227186,\n",
              "        0.25201611, 0.26055336, 0.25807154, 0.25693482, 0.24302508,\n",
              "        0.26353365, 0.25478427, 0.26152437, 0.25511025, 0.27628736,\n",
              "        0.257923  , 0.24051095, 0.24750804, 0.24484025, 0.25317668,\n",
              "        0.2554436 , 0.26591157, 0.25810539, 0.25451304, 0.2535906 ,\n",
              "        0.25638898, 0.25943542, 0.25843555, 0.25381583, 0.25918895,\n",
              "        0.24488875, 0.2539634 , 0.26325421, 0.25700699, 0.26561461,\n",
              "        0.26593277, 0.25018294, 0.26045551, 0.26321708, 0.27617293,\n",
              "        0.25878463, 0.26437827, 0.24911695, 0.26857609, 0.24866487,\n",
              "        0.25102808, 0.25714487, 0.2537739 , 0.23166401, 0.26677012,\n",
              "        0.26392577, 0.26512929, 0.25801161, 0.24831915, 0.24587005,\n",
              "        0.2575345 , 0.25566104, 0.25881251, 0.2475567 , 0.27088235,\n",
              "        0.25258433, 0.24987137, 0.25717967, 0.25478145, 0.25337526,\n",
              "        0.25731051, 0.24970536, 0.25653763, 0.2549949 , 0.25284701,\n",
              "        0.24607684, 0.26516284, 0.24965958, 0.27335256, 0.25724942,\n",
              "        0.24902232, 0.2564187 , 0.25957746, 0.26043637, 0.2409798 ,\n",
              "        0.24971324, 0.25279746, 0.25893238, 0.26783241, 0.26638319,\n",
              "        0.24726223, 0.25939965, 0.25931719, 0.26049849, 0.25379535,\n",
              "        0.26174602, 0.25873971, 0.25041839, 0.27034599, 0.2595811 ,\n",
              "        0.23905069, 0.24311623, 0.25762808, 0.2640189 , 0.26370939,\n",
              "        0.25495851, 0.26063862, 0.2467334 , 0.25133298, 0.26101278,\n",
              "        0.26780199, 0.25569077, 0.25654969, 0.26197217, 0.25848226,\n",
              "        0.24672565, 0.2428053 , 0.25899902, 0.26302859, 0.26262749,\n",
              "        0.26586662, 0.25184339, 0.23682567, 0.25642486, 0.24516809,\n",
              "        0.25255394, 0.25632013, 0.25660795, 0.25326358, 0.25662905,\n",
              "        0.24038093, 0.26837632, 0.25539949, 0.26543799, 0.2615566 ,\n",
              "        0.24547175, 0.24086924, 0.26161734, 0.24327148, 0.27404719,\n",
              "        0.2506436 , 0.2638215 , 0.24405172, 0.25012043, 0.2586216 ,\n",
              "        0.25642994, 0.25786285, 0.26970065, 0.26318417, 0.25812463,\n",
              "        0.26366954, 0.25508434, 0.25812836, 0.25539366, 0.24303654,\n",
              "        0.26603648, 0.26381776, 0.26365612, 0.25540486, 0.26964321,\n",
              "        0.25376422, 0.25545939, 0.2703573 , 0.25725884, 0.24933198,\n",
              "        0.26399899, 0.25876263, 0.26343443, 0.26110458, 0.25966209,\n",
              "        0.25787393, 0.24935694, 0.26653928, 0.26850391, 0.25755019,\n",
              "        0.2506179 , 0.25155989, 0.24582677, 0.26286828, 0.26924846,\n",
              "        0.26028557, 0.26967483, 0.27280184, 0.26305706, 0.25251744,\n",
              "        0.25991096, 0.26500097, 0.25233981, 0.25428396, 0.26188859,\n",
              "        0.26284604, 0.2514868 , 0.26758356, 0.24720561, 0.25811073,\n",
              "        0.25210092, 0.26924527, 0.26972154, 0.25836167, 0.26263359,\n",
              "        0.2619962 , 0.26092816, 0.25911321, 0.24909962, 0.26254415,\n",
              "        0.2543981 , 0.26082571, 0.26227224, 0.26648328, 0.25358091,\n",
              "        0.2598997 , 0.26953619, 0.26614914, 0.26019716, 0.25239213,\n",
              "        0.25509565, 0.26067415, 0.25887575, 0.2592151 , 0.26408597,\n",
              "        0.25992529, 0.25198863, 0.26741072, 0.27014941, 0.25552605,\n",
              "        0.25584441, 0.25457918, 0.246878  , 0.25668157, 0.26673658,\n",
              "        0.25583966, 0.2667036 , 0.25648293, 0.25600577, 0.25832263,\n",
              "        0.25133696, 0.24279167, 0.25771942, 0.2549058 , 0.25513311,\n",
              "        0.25147117, 0.25903162, 0.26454461, 0.25332085, 0.25646276,\n",
              "        0.26346021, 0.24773743, 0.25554415, 0.26248664, 0.26739884,\n",
              "        0.24609601, 0.25257219, 0.25316443, 0.26005416, 0.25137352,\n",
              "        0.25485391, 0.25589746, 0.24948252, 0.25927176, 0.25671374,\n",
              "        0.24580805, 0.24954033, 0.23759495, 0.26797641, 0.26174219,\n",
              "        0.24968078, 0.2568738 , 0.2596468 , 0.2497715 , 0.26550611,\n",
              "        0.25415088, 0.2468691 , 0.25243246, 0.25682615, 0.25434823,\n",
              "        0.25862272, 0.26305952, 0.26647637, 0.24897372, 0.26483093,\n",
              "        0.24977912, 0.26591518, 0.26487321, 0.26785194, 0.26840631,\n",
              "        0.25731806, 0.26021439, 0.25999604, 0.25811759, 0.25376961,\n",
              "        0.26816397, 0.24149221, 0.26390927, 0.24435837, 0.25853828,\n",
              "        0.25742027, 0.24477075, 0.26445842, 0.26205185, 0.25806799,\n",
              "        0.24762493, 0.26627927, 0.25641111, 0.25518632, 0.26777428,\n",
              "        0.24818278, 0.25652884, 0.26943662, 0.25102089, 0.24771114,\n",
              "        0.27500591, 0.2596248 , 0.26744479, 0.26076454, 0.24657509,\n",
              "        0.25681454, 0.25339056, 0.2576702 , 0.25178427, 0.27033432,\n",
              "        0.24652483, 0.2702255 , 0.25367821, 0.26599156, 0.2463906 ,\n",
              "        0.27113172, 0.25242924, 0.25561796, 0.25748861, 0.25235651,\n",
              "        0.25044841, 0.26347518, 0.2531534 , 0.25604123, 0.2605351 ,\n",
              "        0.27046259, 0.25485115, 0.25837809, 0.24701547, 0.2555522 ,\n",
              "        0.26047624, 0.26168888, 0.2400453 , 0.25256537, 0.25215915,\n",
              "        0.2579448 , 0.24974721, 0.23440531, 0.25702029, 0.23173673,\n",
              "        0.25807035, 0.25455088, 0.26459784, 0.23792945, 0.25076599,\n",
              "        0.26369833, 0.24491267, 0.27371443, 0.24682478, 0.24083436,\n",
              "        0.25439918, 0.25995286, 0.25852224, 0.25056262, 0.25800876,\n",
              "        0.25463218, 0.25588133, 0.25765859, 0.24850586, 0.24994286,\n",
              "        0.26094644, 0.25098643, 0.25971636, 0.26823596, 0.25024496,\n",
              "        0.25075664, 0.2536329 , 0.25338387, 0.24453454, 0.24758881,\n",
              "        0.27015337, 0.25106275, 0.26513319, 0.26353434, 0.26668083,\n",
              "        0.26099638, 0.26901638, 0.25033463, 0.26486242, 0.25383665,\n",
              "        0.25574924, 0.26699696, 0.25728792, 0.26411526, 0.25253394,\n",
              "        0.26628735, 0.2586976 , 0.25303974, 0.25746912, 0.26143062,\n",
              "        0.24792131, 0.25450835, 0.26403619, 0.26620659, 0.25977669,\n",
              "        0.26395503, 0.25936061, 0.26481259, 0.25143181, 0.2553296 ,\n",
              "        0.25967429, 0.25878324, 0.2586655 , 0.26549265, 0.26417992,\n",
              "        0.24949455, 0.26356417, 0.26641243, 0.25198127, 0.2600161 ,\n",
              "        0.24782744, 0.2497781 , 0.26565471, 0.25308415, 0.24738888,\n",
              "        0.25563657, 0.27295933, 0.26459091, 0.25663739, 0.26016827,\n",
              "        0.25918077, 0.25333602, 0.27641686, 0.24616382, 0.25658709,\n",
              "        0.2553851 , 0.26204942, 0.25813974, 0.25194545, 0.26871506,\n",
              "        0.25165895, 0.25969572, 0.25144076, 0.24961527, 0.25819607,\n",
              "        0.27180986, 0.26267718, 0.2675545 , 0.26763549, 0.27765267,\n",
              "        0.25171781, 0.26871375, 0.24910342, 0.26400368, 0.2476389 ,\n",
              "        0.26093425, 0.25447272, 0.26304591, 0.25715749, 0.24809781,\n",
              "        0.26509244, 0.24702908, 0.25674841, 0.25414235, 0.24785136,\n",
              "        0.26333352, 0.26312313, 0.24786592, 0.2671938 , 0.25611528,\n",
              "        0.25808423, 0.2555776 , 0.26846823, 0.26160265, 0.26135023,\n",
              "        0.2638285 , 0.25403546, 0.24362803, 0.25803539, 0.26359442,\n",
              "        0.26157393, 0.24439789, 0.25382776, 0.24693937, 0.26058977,\n",
              "        0.26697166, 0.25681453, 0.25962931, 0.25165422, 0.25042263,\n",
              "        0.26414847, 0.26013816, 0.23336287, 0.27067352, 0.25611329,\n",
              "        0.25296433, 0.24315958, 0.24976667, 0.26035259, 0.26168043,\n",
              "        0.26736593, 0.25862804, 0.25293286, 0.25317542, 0.25550395,\n",
              "        0.25420452, 0.25891006, 0.26143541, 0.26512691, 0.25165198,\n",
              "        0.26078604, 0.2582832 , 0.25059184, 0.24574855, 0.25588416,\n",
              "        0.26454815, 0.25545805, 0.24831871, 0.25339647, 0.25052841,\n",
              "        0.25187741, 0.25389963, 0.2530338 , 0.24868725, 0.26326892,\n",
              "        0.26482654, 0.25509937, 0.26397867, 0.25275596, 0.26333172,\n",
              "        0.25869576, 0.26136118, 0.25998188, 0.24889116, 0.25447232,\n",
              "        0.2588514 , 0.26490844, 0.24790924, 0.25949002, 0.25375411,\n",
              "        0.24588234, 0.25457749, 0.24945608, 0.25340914, 0.25654297,\n",
              "        0.26040972, 0.26564826, 0.27041764, 0.25021272, 0.26425889,\n",
              "        0.26627521, 0.24550501, 0.27933979, 0.25610841, 0.25649734,\n",
              "        0.24784471, 0.24553953, 0.26133326, 0.26572261, 0.26581463,\n",
              "        0.27225997, 0.2530517 , 0.25480894, 0.26842303, 0.25465319,\n",
              "        0.2472121 , 0.25600043, 0.25451365, 0.26989267, 0.2579537 ,\n",
              "        0.2629387 , 0.2738675 , 0.25757867, 0.2639346 , 0.25605468,\n",
              "        0.26340454, 0.2586679 , 0.25379165, 0.2462103 , 0.26191957,\n",
              "        0.27228281, 0.2830533 , 0.25813578, 0.26395382, 0.24526009,\n",
              "        0.25815812, 0.24691002, 0.27105084, 0.24818906, 0.24903588,\n",
              "        0.25328095, 0.24777304, 0.26476473, 0.26173979, 0.25528375,\n",
              "        0.26327172, 0.25618722, 0.25690817, 0.25854815, 0.24592346,\n",
              "        0.272668  , 0.25759448, 0.24528726, 0.26882048, 0.256485  ,\n",
              "        0.25980974, 0.24489686, 0.25986118, 0.24379947, 0.25455972,\n",
              "        0.2588741 , 0.26556452, 0.25650082, 0.25654024, 0.25043183,\n",
              "        0.25490064, 0.26653098, 0.25104147, 0.24930648, 0.25650346,\n",
              "        0.2684483 , 0.26760942, 0.25206163, 0.24771316, 0.25697064,\n",
              "        0.25470951, 0.26066268, 0.25090576, 0.26263642, 0.2513842 ,\n",
              "        0.2487111 , 0.2541301 , 0.26648011, 0.25877682, 0.26813033,\n",
              "        0.26994843, 0.25253535, 0.26705634, 0.26442929, 0.26086162,\n",
              "        0.25756189, 0.27853298, 0.25285704, 0.25655228, 0.25383354,\n",
              "        0.26622688, 0.26979749, 0.25903358, 0.24397453, 0.25242839,\n",
              "        0.26908053, 0.26268217, 0.26928665, 0.25576545, 0.26559279,\n",
              "        0.26183817, 0.26243389, 0.2576943 , 0.24453601, 0.26114933,\n",
              "        0.2617037 , 0.2685326 , 0.25383745, 0.25504715, 0.25655782,\n",
              "        0.25180469, 0.25490629, 0.25882004, 0.26392651, 0.26278176,\n",
              "        0.26198502, 0.24847189, 0.25585873, 0.25533149, 0.25688792,\n",
              "        0.26085212, 0.25529596, 0.24488409, 0.26128568, 0.25612651,\n",
              "        0.25882488, 0.25351005, 0.25310451, 0.24404182, 0.24693105,\n",
              "        0.25357148, 0.25471045, 0.25766837, 0.24601374, 0.25162688,\n",
              "        0.24482468, 0.24830387, 0.24413905, 0.26914775, 0.25940946,\n",
              "        0.26790904, 0.25017098, 0.26191809, 0.26872806, 0.25814474,\n",
              "        0.25180985, 0.26842859, 0.25439896, 0.25705753, 0.25847179,\n",
              "        0.26636404, 0.2561621 , 0.25133392, 0.26027641, 0.2619139 ,\n",
              "        0.25431064, 0.26728699, 0.24540461, 0.25676167, 0.25556125,\n",
              "        0.26359232, 0.25296322, 0.25248451, 0.26536804, 0.25071203,\n",
              "        0.26378543, 0.25397417, 0.2569719 , 0.25582164, 0.25678895,\n",
              "        0.25244446, 0.24825681, 0.24420709, 0.26313179, 0.24737013,\n",
              "        0.25408962, 0.25506956, 0.26642626, 0.25848097, 0.25690799,\n",
              "        0.24550057, 0.25316043, 0.25713598, 0.27026013, 0.25583442]))"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bootstrap_ate(\n",
        "      df, stratified_ate, B=1000, propensity_col=\"prop\", strata_bounds=20\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "102926f0",
      "metadata": {},
      "source": [
        "It seems the empirical results are aligned with our derivations above: $V_{IPW^*} \\geq V_{STRAT}$. Let's run simulations to compare their behavior under different values of $n$ and different number of strata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8e81040",
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20f3f040",
      "metadata": {},
      "source": [
        "## Balance checks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "406c72e9",
      "metadata": {},
      "source": [
        "Let's take a look at the rebalancing property of $e(X)$. A standard procedure when studying treatment effects is to run a **balance check**. A balance check analyzes the covariate distributions between the treatment and control groups. In a RCT, by design, the covariate distributions among the groups are the same. This amounts to say that the groups are comparable and they are good counterfactuals of each other. In practice, however, sampling errors and/or other shenanigans might threaten the balance of our setting. This is why it is always a good practice to check whether the covariates in our group are balanced or not. If they are not, then we might need to correct the imbalance.\n",
        "\n",
        "There are multiple ways to help determine whether the sample is imbalanced. In this exercise we will focus on **standardized mean differences**. The concept is very simple: we compute the difference in means for each covariate in the treatment and control groups, and normalize by the standard error of the difference. For a given covariate $X_{ij}$:\n",
        "\n",
        "$$S = \\frac{\\mu_{j1} - \\mu_{j0}}{\\sqrt{\\sigma_{j1}^2 + \\sigma_{j0}^2}}$$\n",
        "\n",
        "And this can be estimated straight-forwardly with:\n",
        "\n",
        "$$\\hat{S} = \\frac{\\bar{X}_{j1} - \\bar{X}_{j0}}{\\sqrt{s_{j1}^2 + s_{j0}^2}}$$\n",
        "\n",
        "where $s_{jw}^2$ is the sample variance of covariate $X_j$ for units in the treatment group $w$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a54fefa",
      "metadata": {},
      "source": [
        "Let's simulate our RCT setting again and check the balance.\n",
        "\n",
        "\\begin{align*}\n",
        "i &= 1,\\ldots, n\\\\\n",
        "X_{ij} &\\overset{iid}{\\sim} \\mathcal{U}(-1, 1), \\; j \\in \\{1,2,3\\}\\\\\n",
        "W_i &\\overset{iid}{\\sim} Ber(p)\\\\\n",
        "Y_i &= \\text{exp}(X_{1i} + X_{2i}) + 0.25 W_i  \n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "1b8b3db8",
      "metadata": {},
      "outputs": [],
      "source": [
        "n = 100_000   # sample size\n",
        "p = 0.5    # Bernoulli probability for W\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# X_ij ~ U(-1, 1) iid for j in {1, 2, 3}\n",
        "rct_X = np.random.uniform(-1, 1, size=(n, 3))\n",
        "\n",
        "# W_i ~ Ber(p) iid\n",
        "rct_W = np.random.binomial(1, p, size=n)\n",
        "\n",
        "# Y_i(W_i) = exp(X_{i,1} + X_{i,2}) + 0.25 * W_i\n",
        "rct_Y = np.exp(rct_X[:, 0] + rct_X[:, 1]) + 0.25 * rct_W\n",
        "\n",
        "rct_df = pd.DataFrame({\n",
        "    \"Y\": rct_Y,\n",
        "    \"W\": rct_W,\n",
        "    \"X1\": rct_X[:, 0],\n",
        "    \"X2\": rct_X[:, 1],\n",
        "    \"X3\": rct_X[:, 2],\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "61bdd64d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Y</th>\n",
              "      <th>W</th>\n",
              "      <th>X1</th>\n",
              "      <th>X2</th>\n",
              "      <th>X3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.916516</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.250920</td>\n",
              "      <td>0.901429</td>\n",
              "      <td>0.463988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.612231</td>\n",
              "      <td>0</td>\n",
              "      <td>0.197317</td>\n",
              "      <td>-0.687963</td>\n",
              "      <td>-0.688011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.859435</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.883833</td>\n",
              "      <td>0.732352</td>\n",
              "      <td>0.202230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.581185</td>\n",
              "      <td>0</td>\n",
              "      <td>0.416145</td>\n",
              "      <td>-0.958831</td>\n",
              "      <td>0.939820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.343697</td>\n",
              "      <td>1</td>\n",
              "      <td>0.664885</td>\n",
              "      <td>-0.575322</td>\n",
              "      <td>-0.636350</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Y  W        X1        X2        X3\n",
              "0 1.916516  0 -0.250920  0.901429  0.463988\n",
              "1 0.612231  0  0.197317 -0.687963 -0.688011\n",
              "2 0.859435  0 -0.883833  0.732352  0.202230\n",
              "3 0.581185  0  0.416145 -0.958831  0.939820\n",
              "4 1.343697  1  0.664885 -0.575322 -0.636350"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rct_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "66e8019f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def standardized_mean_difference(\n",
        "    df,\n",
        "    covariates,\n",
        "    group_col=\"W\",\n",
        "    group1=1,\n",
        "    group0=0,\n",
        "    ddof=1,\n",
        "    propensity_col=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute the estimated standardized mean difference (SMD) for each covariate.\n",
        "\n",
        "    Ŝ_j = (X̄_{j1} - X̄_{j0}) / sqrt(s²_{j1} + s²_{j0})\n",
        "\n",
        "    If propensity_col is provided, means and variances are computed with IPW\n",
        "    weights: weight 1/e(X) in group 1 and 1/(1-e(X)) in group 0.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        DataFrame containing covariates and group indicator.\n",
        "    covariates : list of str\n",
        "        Column names of covariates to compute SMD for.\n",
        "    group_col : str, default \"W\"\n",
        "        Name of the treatment/group column (binary).\n",
        "    group1 : scalar, default 1\n",
        "        Value denoting group 1 (e.g. treatment).\n",
        "    group0 : scalar, default 0\n",
        "        Value denoting group 0 (e.g. control).\n",
        "    ddof : int, default 1\n",
        "        Delta degrees of freedom for sample variance (unweighted only).\n",
        "    propensity_col : str or None, default None\n",
        "        If provided, name of column with propensity score e(X) = P(W=1|X).\n",
        "        Observations are reweighted by 1/e in group 1 and 1/(1-e) in group 0.\n",
        "    propensity_clip : tuple of float, default (1e-9, 1-1e-9)\n",
        "        Bounds to clip propensity scores when computing weights (avoids division by zero).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        Index = covariate names, values = SMD (float). NaN if denominator is 0.\n",
        "    \"\"\"\n",
        "    mask1 = (df[group_col] == group1)\n",
        "    mask0 = (df[group_col] == group0)\n",
        "    out = pd.Series(index=covariates, dtype=float)\n",
        "\n",
        "    if propensity_col is None:\n",
        "        for col in covariates:\n",
        "            x1 = df.loc[mask1, col].values\n",
        "            x0 = df.loc[mask0, col].values\n",
        "            m1, m0 = np.mean(x1), np.mean(x0)\n",
        "            v1 = np.var(x1, ddof=ddof)\n",
        "            v0 = np.var(x0, ddof=ddof)\n",
        "            denom = np.sqrt(v1 + v0)\n",
        "            if denom <= 0:\n",
        "                out[col] = np.nan\n",
        "            else:\n",
        "                out[col] = (m1 - m0) / denom\n",
        "        return out\n",
        "\n",
        "    e = df[propensity_col].values\n",
        "    w1 = np.where(mask1, 1.0 / e, 0.0)\n",
        "    w0 = np.where(mask0, 1.0 / (1.0 - e), 0.0)\n",
        "    for col in covariates:\n",
        "        x = df[col].values\n",
        "        sw1 = np.sum(w1)\n",
        "        sw0 = np.sum(w0)\n",
        "        m1 = np.sum(w1 * x) / sw1 if sw1 > 0 else np.nan\n",
        "        m0 = np.sum(w0 * x) / sw0 if sw0 > 0 else np.nan\n",
        "        if np.isnan(m1) or np.isnan(m0):\n",
        "            out[col] = np.nan\n",
        "            continue\n",
        "        v1 = np.sum(w1 * (x - m1) ** 2) / sw1\n",
        "        v0 = np.sum(w0 * (x - m0) ** 2) / sw0\n",
        "        denom = np.sqrt(v1 + v0)\n",
        "        if denom <= 0:\n",
        "            out[col] = np.nan\n",
        "        else:\n",
        "            out[col] = (m1 - m0) / denom\n",
        "    return out\n",
        "\n",
        "\n",
        "def plot_balance(\n",
        "    df,\n",
        "    covariates,\n",
        "    group_col=\"W\",\n",
        "    group1=1,\n",
        "    group0=0,\n",
        "    propensity_col=None,\n",
        "    variable_labels=None,\n",
        "    xlim=(-0.25, 0.25),\n",
        "    ax=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Create a balance (love) plot of standardized mean differences across groups.\n",
        "\n",
        "    For each covariate, computes SMD = (mean_1 - mean_0) / sqrt(var_1 + var_0)\n",
        "    and plots a point. Optionally reweight observations by inverse propensity\n",
        "    (propensity_col) so balance is assessed in the reweighted population.\n",
        "    Reference lines: solid at 0, dashed at ±0.10, dotted at ±0.25.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        DataFrame with covariates and group column.\n",
        "    covariates : list of str\n",
        "        Column names to assess for balance.\n",
        "    group_col : str, default \"W\"\n",
        "        Name of the treatment/group column.\n",
        "    group1 : scalar, default 1\n",
        "        Value for group 1 (e.g. treatment).\n",
        "    group0 : scalar, default 0\n",
        "        Value for group 0 (e.g. control).\n",
        "    propensity_col : str or None, default None\n",
        "        If provided, name of column in df containing propensity score e(X).\n",
        "        Balance is computed using IPW weights (1/e for treated, 1/(1-e) for control).\n",
        "    variable_labels : dict or None, default None\n",
        "        Optional mapping from column name to display label (e.g. {\"X1\": \"Account age (days)\"}).\n",
        "        If None, column names are used.\n",
        "    xlim : tuple of float, default (-0.25, 0.25)\n",
        "        x-axis limits (Standardized Difference).\n",
        "    ax : matplotlib.axes.Axes or None, default None\n",
        "        Axes to draw on. If None, creates a new figure and axes.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    matplotlib.axes.Axes\n",
        "    \"\"\"\n",
        "    smd = standardized_mean_difference(\n",
        "        df,\n",
        "        covariates,\n",
        "        group_col=group_col,\n",
        "        group1=group1,\n",
        "        group0=group0,\n",
        "        propensity_col=propensity_col,\n",
        "    )\n",
        "    labels = [variable_labels.get(c, c) if variable_labels else c for c in covariates]\n",
        "    y_pos = np.arange(len(covariates))[::-1]  # top = first variable\n",
        "\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(6, max(4, len(covariates) * 0.35)))\n",
        "    ax.set_xlim(xlim)\n",
        "    ax.set_ylim(-0.5, len(covariates) - 0.5)\n",
        "    ax.set_yticks(y_pos)\n",
        "    ax.set_yticklabels(labels, fontsize=10)\n",
        "    ax.set_xlabel(\"Standardized Difference\", fontsize=11)\n",
        "    ax.set_ylabel(\"Variable\", fontsize=11)\n",
        "    if propensity_col is not None:\n",
        "        ax.set_title(\"Balance (IPW reweighted)\", fontsize=11)\n",
        "    ax.axvline(0, color=\"black\", lw=1.5, zorder=0)\n",
        "    ax.axvline(-0.10, color=\"black\", ls=\"--\", lw=1, zorder=0)\n",
        "    ax.axvline(0.10, color=\"black\", ls=\"--\", lw=1, zorder=0)\n",
        "    ax.axvline(-0.25, color=\"black\", ls=\":\", lw=1, zorder=0)\n",
        "    ax.axvline(0.25, color=\"black\", ls=\":\", lw=1, zorder=0)\n",
        "    ax.scatter(smd.values, y_pos, color=\"purple\", s=50, zorder=2, edgecolors=\"none\")\n",
        "    ax.spines[\"top\"].set_visible(False)\n",
        "    ax.spines[\"right\"].set_visible(False)\n",
        "    plt.tight_layout()\n",
        "    return ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "1f0909ad",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Balance plot for X1, X2, X3 (treatment W)\n",
        "plot_balance(\n",
        "    rct_df,\n",
        "    covariates=[\"X1\", \"X2\", \"X3\"],\n",
        "    group_col=\"W\",\n",
        "    variable_labels={\"X1\": \"X1\", \"X2\": \"X2\", \"X3\": \"X3\"},  # or nicer names\n",
        "    xlim=(-0.5, 0.5)\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa8dffca",
      "metadata": {},
      "source": [
        "As expected, covariates are balanced across groups in the RCT setting. What about in the observational dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "68290a0f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Balance plot for X1, X2, X3 (treatment W)\n",
        "plot_balance(\n",
        "    df,\n",
        "    covariates=[\"X1\", \"X2\", \"X3\"],\n",
        "    group_col=\"W\",\n",
        "    variable_labels={\"X1\": \"X1\", \"X2\": \"X2\", \"X3\": \"X3\"},  # or nicer names\n",
        "    xlim=(-0.5, 0.5)\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8549ff4",
      "metadata": {},
      "source": [
        "Balance looks pretty bad. As a rule of thumb, covariates with a standardized difference below 0.10 can be considered balanced across groups.\n",
        "\n",
        "But what would happen if we reweigh the observations using the (inverse of the) propensity score?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "3bafd949",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Balance plot for X1, X2, X3 (treatment W)\n",
        "plot_balance(\n",
        "    df,\n",
        "    covariates=[\"X1\", \"X2\", \"X3\"],\n",
        "    group_col=\"W\",\n",
        "    variable_labels={\"X1\": \"X1\", \"X2\": \"X2\", \"X3\": \"X3\"},  # or nicer names\n",
        "    xlim=(-0.5, 0.5),\n",
        "    propensity_col='prop'\n",
        ")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml.ci.uba.2026.venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
